{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a5744ba",
   "metadata": {},
   "source": [
    "User Manual for Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preparing processes\n",
    "\n",
    "## In this part, We import packages that we need to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from mlxtend.evaluate import bias_variance_decomp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, we transfer catagorical data to numerical representations to run regression\n",
    "\n",
    "* For catagorical data ShelveLoc, we tranfer Good to 2, Meduim to 1 and bad to 0. \n",
    "\n",
    "* For catagorical data Urban and US, we transfer Yes to 1, No to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df = pd.DataFrame(pd.read_csv(\"Carseats.csv\"))\n",
    "# transform classes into numerical value\n",
    "df['ShelveLoc'].replace(['Good', 'Medium', 'Bad'],[2,1,0], inplace=True)\n",
    "df['Urban'].replace(['Yes', 'No'], [1,0], inplace = True)\n",
    "df['US'].replace(['Yes', 'No'], [1,0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then we split training dataset and testing dataset.\n",
    "\n",
    "### Approach 1: utilize pandas\n",
    "\n",
    "We randomly pick 80% data as training data, other 20% as test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder indexes\n",
    "df = df.sample(frac=1.0)\n",
    "df = df.reset_index()\n",
    "# drop column of index\n",
    "df = df.drop(\"index\", axis = 1)\n",
    "\n",
    "# divide training dataset and testing dataset\n",
    "# train\n",
    "train = df.sample(frac=0.8)\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "train_x = train.drop([\"Sales\"], axis=1)\n",
    "train_y = train[\"Sales\"]\n",
    "#test\n",
    "test = df[~df.index.isin(train.index)]\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "test_x = test.drop([\"Sales\"], axis=1)\n",
    "test_y = test[\"Sales\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### approach 2: use train_test_split method in package sklearn.model_selection\n",
    "\n",
    "the effect is equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # 划分数据集的模块\n",
    "train_x, test_x, train_y, test_y = train_test_split(df.drop([\"Sales\"], axis=1),df[\"Sales\"],test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a) illustrate data statistics\n",
    "\n",
    "## We first describe the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape, test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then we plot histograms\n",
    "\n",
    "### Sales\n",
    "\n",
    "else are performed in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(df.iloc[:,0],edgecolor=\"white\")\n",
    "ax.set_title('Histogram For Sales')\n",
    "ax.set_xlabel('Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grow and then plot the decision tree\n",
    "\n",
    "## parameter explanation\n",
    "\n",
    "As we use SSE (sum of squared error) as loss, we adopt the inherited \"squared error\" approach since it uses MSE, which is the same as using SSE (MSE = SSE/n), as default\n",
    "\n",
    "## formula under the model\n",
    "\n",
    "### for each leaf\n",
    "\n",
    "e := MSE = $\\frac{sum_{i=1}^{n} (y_i - \\bar{y})^2} {n}$\n",
    "\n",
    "### Total Mse is computed as:\n",
    "\n",
    "$ S = \\sum_{m \\in leaves} e_m $\n",
    "\n",
    "### After training model, we have Training MSE. Then we can also compute Test MSE with the trained model.\n",
    "\n",
    "## coding\n",
    "\n",
    "### preparing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute training and testing MSE\n",
    "def compute_mse(clf,x,y):\n",
    "    predicted = clf.predict(x)\n",
    "    mse = 0\n",
    "    for i in range(len(x)):\n",
    "        error_sqr = (y[i] - predicted[i])**2\n",
    "        mse += error_sqr\n",
    "    mse /= len(x)\n",
    "    return mse\n",
    "\n",
    "# train regression tree with max_depth, min_samples_leaf and then return the train_score, train_mse, test_score and test_mse\n",
    "def compute_accuracy(max_depth, min_samples_leaf):\n",
    "    global train_x, train_y, test_x, test_y\n",
    "    clf = tree.DecisionTreeRegressor(max_depth=max_depth,min_samples_leaf = min_samples_leaf)\n",
    "    clf = clf.fit(train_x, train_y)\n",
    "\n",
    "    # compute training accuracy\n",
    "    train_score = clf.score(train_x, train_y) # return R^2 of fitting\n",
    "    test_score = clf.score(test_x, test_y) # return R^2 of fitting\n",
    "\n",
    "    train_mse = compute_mse(clf,train_x, train_y)\n",
    "    test_mse = compute_mse(clf,test_x,test_y)\n",
    "\n",
    "    return clf, train_score, train_mse, test_score, test_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define function to plot tree given max depth and min sample leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### approach 1 to plot tree: using plot_tree method in tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list()\n",
    "def output_result(max_depth, min_samples_leaf):\n",
    "    clf, train_score, train_mse, test_score, test_mse = compute_accuracy(max_depth=max_depth,min_samples_leaf=min_samples_leaf)\n",
    "    \n",
    "    # print the train and testing accuracy\n",
    "    name = f\"depth - {max_depth} minimum samples leaf - {min_samples_leaf}\" + \".jpg\"\n",
    "    print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"-----------------Regression Tree with maximum depth = {max_depth} and minimun number of sample in a leaf = {min_samples_leaf}----------------\")\n",
    "    print(\"Train accuracy: \" , train_score , \"\\nTraining MSE: \" , train_mse , \"\\nTesting accuracy: \" , test_score , \"\\nTest MSE: \" , test_mse)\n",
    "    # print(\"------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "    # plot the tree\n",
    "    plt.figure(figsize=(25,25))\n",
    "\n",
    "    tree.plot_tree(clf,fontsize=9)\n",
    "    \n",
    "    plt.savefig(name)\n",
    "    result.append([max_depth, min_samples_leaf, train_score, train_mse, test_score, test_mse])\n",
    "    print(\"------------------------------------------------------------------------------------------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### approach 2 to plot tree: using package graphviz.Source(dot_data)\n",
    "\n",
    "the example is from a classification tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "dot_data = tree.export_graphviz(clf #训练好的模型\n",
    "                                ,out_file = None\n",
    "                                ,feature_names= feature_name\n",
    "                                ,class_names=[\"琴酒\",\"雪莉\",\"贝尔摩德\"]\n",
    "                                ,filled=True #进行颜色填充\n",
    "                                ,rounded=True #树节点的形状控制\n",
    ")\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## draw trees for models:\n",
    "\n",
    "* model1: max depth = 3 and min sample leaf = 5\n",
    "\n",
    "* model2: max depth = 3 and min sample leaf = 10\n",
    "\n",
    "* model3: max depth = 3 and min sample leaf = 15\n",
    "\n",
    "* model4: max depth = 5 and min sample leaf = 10\n",
    "\n",
    "* model5: max depth = 7 and min sample leaf = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_result(3,5)\n",
    "# output_result(3,10)\n",
    "# output_result(3,15)\n",
    "# output_result(5,10)\n",
    "# output_result(7,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To sum up, we have training result and testing result as follows:\n",
    "\n",
    "|max_depth | min_samples_leaf | train_score | train_mse | test_score | test_mse |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "|3 | 5 | 0.5513142619684648 | 3.469907690714441 | 0.3429246728518772 | 3.89318955376371|\n",
    "|3 | 10 | 0.5456586169740507 | 3.5136455776108786 | 0.3236975350499671 | 4.007110878985372|\n",
    "|3 | 15 | 0.5341587592558981 | 3.602579638482532 | 0.30315024242726696 | 4.128854158167713|\n",
    "|5 | 10 | 0.6776681795467121 | 2.4927506447157017 | 0.5338846321452615 | 2.7617464939019363|\n",
    "|7 | 10 | 0.7260161022281123 | 2.118852357958776 | 0.6196534289927718 | 2.253563991638637|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## if we want to do with pruning manually and get correlated information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征重要性\n",
    "clf.feature_importances_# 查看每一个特征对分类的贡献率\n",
    "[*zip(feature_name,clf.feature_importances_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这里，我们发现每一次运行程序时，返回的准确率都不相同，这是因为sklearn每次都在全部的特征中选取若干个特征建立一棵树\n",
    "# 最后选择准确率最高的决策树返回，如果我们添加上random_state参数，那么sklearn每一次建立决策树使用的特征都相同，返回的\n",
    "# 预测分数也会一样clf = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=30)\n",
    "clf = clf.fit(Xtrain, Ytrain)\n",
    "score = clf.score(Xtest, Ytest) #返回预测的准确度\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knowing the basic information, we need to prune tree to reduce overfitting\n",
    "\n",
    "#### check if overfitting for training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score_train = clf.score(Xtrain, Ytrain)\n",
    "# 对于训练数据集的预测分数为100%，也就是过拟合了，需要我们做剪枝处理\n",
    "score_train \n",
    "\n",
    "# if overfitting, we pose limit to min_samples_leaf and min_samples_split\n",
    "\n",
    "clf1 = tree.DecisionTreeClassifier(criterion=\"entropy\"\n",
    "                                  ,random_state=30\n",
    "                                  ,splitter=\"random\"\n",
    "                                  ,max_depth=3\n",
    "                                  ,min_samples_leaf=10 #一个节点分支后，每一个子节点至少包含10个样本\n",
    "                                  ,min_samples_split=10 #一个节点至少包含10个样本才会分支\n",
    ")\n",
    "clf1=clf1.fit(Xtrain,Ytrain)#拟合模型\n",
    "dot_data=tree.export_graphviz(clf,feature_names=feature_name,class_names=[\"琴酒\",\"雪莉\",\"贝尔摩德\"],filled=True,rounded=True)\n",
    "graph=graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 精修树的特征\n",
    "\n",
    "max_features & min_impurity_decrease\n",
    "\n",
    "一般max_depth使用，用作树的”精修“。max_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃。和max_depth异曲同工，max_features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以使用的特征数量而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。\n",
    "\n",
    "min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生。这是在0.19版本中更新的功能，在0.19版本之前时使用min_impurity_split。\n",
    "\n",
    "#### get the best hyper-parameter: draw learning plot (error v.s. hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "test = []\n",
    "for i in range(10):\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=i+1\n",
    "                                      ,criterion=\"entropy\"\n",
    "                                      ,random_state=30\n",
    "                                      ,splitter=\"random\"\n",
    "    )\n",
    "    clf = clf.fit(Xtrain, Ytrain)\n",
    "    score = clf.score(Xtest, Ytest)\n",
    "    test.append(score)\n",
    "plt.plot(range(1,11),test,color=\"red\",label=\"max_depth\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bagging for trees\n",
    "\n",
    "## Illustration\n",
    "\n",
    "Here we try growing bagging tree for different depth and different number of trees.\n",
    "\n",
    "## Growing steps\n",
    "\n",
    "### Step 1:\n",
    "\n",
    "Sample records with replacement, i.e. bootstrap training data to obtain several diverse training datasets\n",
    "\n",
    "### Step2:\n",
    "\n",
    "Fit the full model to each resampled training dataset.\n",
    "\n",
    "### Step3:\n",
    "\n",
    "Aggregate the predictions of all single trees. The average value is put as the prediction\n",
    "\n",
    "## Coding\n",
    "\n",
    "### We first define the function to output result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_bagging = list()\n",
    "def output_bagging(number_of_tree, max_depth):\n",
    "    global train_x, train_y, test_x, test_y\n",
    "    base_clf = tree.DecisionTreeRegressor(max_depth=max_depth,min_samples_leaf = 5)\n",
    "    clf = BaggingRegressor(base_estimator=base_clf,oob_score=True,n_estimators=number_of_tree,random_state=42)\n",
    "    clf = clf.fit(train_x,train_y)\n",
    "\n",
    "    train_mse = compute_mse(clf,train_x,train_y)\n",
    "    test_mse = compute_mse(clf,test_x,test_y)\n",
    "    accuracy = clf.oob_score_\n",
    "\n",
    "    print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"-----------------Regression Tree with maximum depth = {max_depth} and number of bagging trees = {number_of_tree}----------------\")\n",
    "    print(\"Training MSE: \" , train_mse , \"\\nTest MSE: \" , test_mse, \"\\nOOB Score: \", accuracy)\n",
    "    print(\"------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "    result_bagging.append([max_depth, number_of_tree, train_mse,test_mse,accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try different depth and different number of tree\n",
    "\n",
    "* maximun depth = 10, number of tree = 10\n",
    "\n",
    "* maximun depth = 10, number of tree = 20\n",
    "\n",
    "* maximun depth = 10, number of tree = 50\n",
    "\n",
    "* maximun depth = 10, number of tree = 100\n",
    "\n",
    "* maximun depth = 10, number of tree = 200\n",
    "\n",
    "* maximun depth = 3, number of tree = 50\n",
    "\n",
    "* maximun depth = 5, number of tree = 50\n",
    "\n",
    "* maximun depth = 15, number of tree = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_num_tree = [10,20,50,100,200]\n",
    "for tree_num in try_num_tree:\n",
    "    output_bagging(number_of_tree = tree_num, max_depth = 10)\n",
    "\n",
    "output_bagging(number_of_tree=50,max_depth=3)\n",
    "\n",
    "output_bagging(number_of_tree=50,max_depth=5)\n",
    "\n",
    "output_bagging(number_of_tree=50,max_depth=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To sum up， we summarize the output as table:\n",
    "\n",
    "|max_depth | number_of_tree | train_mse | test_mse | OOB Score|\n",
    "| --- | --- | --- | --- | --- |\n",
    "|10 | 10 | 1.4282862942484493 | 1.4792816075410418 | 0.5740700266134335|\n",
    "|10 | 20 | 1.2727152828703814 | 1.3233867244240793 | 0.6111830279416357|\n",
    "|10 | 50 | 1.2498169388765552 | 1.4035065708634766 | 0.6341632588258435|\n",
    "|10 | 100 | 1.2242041274803506 | 1.330610859881188 | 0.645983255682975|\n",
    "|10 | 200 | 1.216718600689651 | 1.3483932890600459 | 0.6468993072337046|\n",
    "|3 | 50 | 2.966433598730782 | 3.213220935261924 | 0.5076018639027922|\n",
    "|5 | 50 | 1.6773229717753961 | 1.841179913558071 | 0.6063411599122327|\n",
    "|15 | 50 | 1.2492917987531191 | 1.4027737677334209 | 0.6343230238269876|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d) Random forest\n",
    "\n",
    "## Description:\n",
    "\n",
    "### Basic idea: \n",
    "\n",
    "constructing several diverse decision trees and then combine their predicting results. By this model, we introduce randomness into data-level.\n",
    "\n",
    "### aim: \n",
    "\n",
    "to reduce correlation among the trees produced by bagging, we introduce split-attribute randomization into model: random forest\n",
    "\t\n",
    "## building algorithm:\n",
    "\n",
    "each time a split is performed (search for the split attribute is limited to a random subset of m of the N attributes) \n",
    "\n",
    "That is, the number of feature a sub-tree use, the number of data samples a tree uses is limited. This reduces correlation among trees.\n",
    "\n",
    "## formula\n",
    "\n",
    "###\tfor regression tree: $m = \\frac{N}{3}$\n",
    "### for classification tree: $m=\\sqrt N$\n",
    "\n",
    "## Coding\n",
    "\n",
    "### first define function to count error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rf = list()\n",
    "def output_rf(number_of_tree, m):\n",
    "    global train_x, train_y, test_x, test_y\n",
    "    clf = RandomForestRegressor(max_features=m, n_estimators=number_of_tree, oob_score=True, random_state=123)\n",
    "    clf = clf.fit(train_x,train_y)\n",
    "\n",
    "    train_mse = compute_mse(clf,train_x,train_y)\n",
    "    test_mse = compute_mse(clf,test_x,test_y)\n",
    "\n",
    "\n",
    "    print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"-----------------maximum number of features = {m} and number of growing tree = {number_of_tree}----------------\")\n",
    "    print(\"Training MSE: \" , train_mse , \"\\nTest MSE: \" , test_mse)\n",
    "    try:\n",
    "        accuracy = clf.oob_score_\n",
    "        print(\"\\nOOB Score: \", accuracy)\n",
    "    except:\n",
    "        pass\n",
    "    print(\"------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "    try:\n",
    "        result_rf.append([number_of_tree, m, train_mse,test_mse,accuracy])\n",
    "    except:\n",
    "        result_rf.append([number_of_tree, m, train_mse,test_mse])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we try different number of trees and values of m\n",
    "\n",
    "* number of tree = 10,29,...,100, while m = 10\n",
    "\n",
    "* number of tree = 50, while m = 2, 3, ... , 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10,110,10):\n",
    "    output_rf(i,10)\n",
    "\n",
    "for i in range(2,10):\n",
    "    output_rf(50,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To sum up, the result of model is:\n",
    "\n",
    "|number_of_tree | m | train_mse | test_mse | accuracy|\n",
    "| --- | --- | --- | --- | --- |\n",
    "|10 | 10 | 0.5335200200000003 | 0.7871440700000001 | 0.3467342858099355|\n",
    "|20 | 10 | 0.4364876541666668 | 0.7240717475000001 | 0.5999057144983408|\n",
    "|30 | 10 | 0.4098352755555559 | 0.7156564111111117 | 0.6334500431555337|\n",
    "|40 | 10 | 0.407899104375 | 0.7187785887499998 | 0.6351655397341303|\n",
    "|50 | 10 | 0.39257554666666655 | 0.6859994412 | 0.6506544917751644|\n",
    "|60 | 10 | 0.3858063809259258 | 0.6635371325 | 0.6541760861143977|\n",
    "|70 | 10 | 0.383716292244898 | 0.6566139267346943 | 0.6561511569752639|\n",
    "|80 | 10 | 0.3812138177083334 | 0.6525010493750003 | 0.6556321867315669|\n",
    "|90 | 10 | 0.3779110195061731 | 0.6414826459259259 | 0.6551598837463128|\n",
    "|100 | 10 | 0.37467921310000035 | 0.6449070439000002 | 0.6557394687114719|\n",
    "|50 | 2 | 0.5064935771999997 | 0.8488472079999996 | 0.526867202635738|\n",
    "|50 | 3 | 0.43809759706666673 | 0.8125209203999998 | 0.5871346553018497|\n",
    "|50 | 4 | 0.4157524953333331 | 0.6809749019999998 | 0.6153834534513313|\n",
    "|50 | 5 | 0.3765126521333334 | 0.6351890164000001 | 0.657199068199613|\n",
    "|50 | 6 | 0.3855411556000001 | 0.6474594047999999 | 0.6502836382766455|\n",
    "|50 | 7 | 0.3644490563999996 | 0.6212765356000001 | 0.6638710749153269|\n",
    "|50 | 8 | 0.39347622453333364 | 0.7214084211999999 | 0.6408934836836202|\n",
    "|50 | 9 | 0.4100130346666663 | 0.6858754636000003 | 0.6328933984156652|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (e) $Bias^2$ and Variance curve\n",
    "\n",
    "## Steps\n",
    "\n",
    "* We train 10 models with different value of features. The number of features are 10, 20, ..., 100\n",
    "\n",
    "* Then we compute the error rate based on the original test dataset.\n",
    "\n",
    "## Formula\n",
    "\n",
    "### $Bias^2$\n",
    "\n",
    "$$ Bias^2 = E_{(x,y)} [(\\bar{h} (x) - t(x) )^2] $$\n",
    "\n",
    "### Variance\n",
    "\n",
    "$$ Variance = E_{(x,y)} [(h_D (x) - \\bar{h} (x) )^2] $$\n",
    "\n",
    "## Coding\n",
    "\n",
    "### We first compute bias square and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_sqr_list = list()\n",
    "var_list = list()\n",
    "\n",
    "X_train = train_x.values\n",
    "X_test = test_x.values\n",
    "y_train = train_y.values\n",
    "y_test = test_y.values\n",
    "\n",
    "for i in range(10,110,10):\n",
    "    rf = RandomForestRegressor(max_features = \"sqrt\", random_state = 123, n_estimators = i)\n",
    "    rf.fit(train_x, train_y)\n",
    "    avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(rf, X_train, y_train, X_test, y_test, loss='mse',random_seed=123)\n",
    "    bias_sqr_list.append(avg_bias**2)\n",
    "    var_list.append(avg_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we plot the $Bias^2$ graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(10,110,10), bias_sqr_list, linewidth=2.0)\n",
    "ax.scatter(range(10,110,10),bias_sqr_list)\n",
    "ax.set(xlim=(10, 100), xticks=np.arange(10, 100,10), \n",
    "       ylim=(0, 4), yticks=np.arange(1, 4))\n",
    "ax.set_title('Bias Square with different number of trees')\n",
    "ax.set_ylabel('Bias square')\n",
    "ax.set_xlabel('Number of Trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then plot the graph for variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(10,110,10), var_list, linewidth=2.0)\n",
    "ax.scatter(range(10,110,10),var_list)\n",
    "ax.set(xlim=(10, 100), xticks=np.arange(10, 100,10), \n",
    "       ylim=(0, 1), yticks=np.arange(0, 1))\n",
    "ax.set_title('Variance with different number of trees')\n",
    "ax.set_ylabel('Variance')\n",
    "ax.set_xlabel('Number of Trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the two graphs:\n",
    "\n",
    "### For Bias:\n",
    "\n",
    "In the graph, we can observe that there is no clear relationship between variance and bias. Also, for bias square, It does not wave a lot with different number of trees.\n",
    "\n",
    "### For Variance:\n",
    "\n",
    "From the graph above, we can draw that the variance of forest descendsmonotinously with the increase of number_of_trees. That is, the increase of tree number can help reduce variance to somehow reduce the total error."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cdab946679e508134cee8b85c485d183daa9e217c64a0dc260a893ba5557ea59"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
